{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bab87ae",
   "metadata": {},
   "source": [
    "# Global stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c130e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75bb4864",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd42267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_file_with_keyword(folder_path, filename_search):\n",
    "    file_path = os.path.join(folder_path, f\"{filename_search}\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read and parse the data from the file\n",
    "            # Assume the data format includes 'q_nm^-1' and 'I'\n",
    "            data = {\n",
    "                \"q_nm^-1\": [],  # List of q values\n",
    "                \"I\": [],  # List of corresponding I values\n",
    "                \"sigma\": []     # List of corresponding sigma values\n",
    "            }\n",
    "\n",
    "            for line in file:\n",
    "                if line.startswith(\"#\"):  # Skip comments if any\n",
    "                    continue\n",
    "\n",
    "                parts = line.strip().split()  # Assuming data is whitespace-separated\n",
    "                if len(parts) >= 2:\n",
    "                    q_value = float(parts[0])\n",
    "                    I_value = float(parts[1])\n",
    "                    sigma = float(parts[2])\n",
    "                    data[\"q_nm^-1\"].append(q_value)\n",
    "                    data[\"I\"].append(I_value)\n",
    "                    data[\"sigma\"].append(sigma)\n",
    "\n",
    "            return data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_data_from_folder(folder_path, filename_search):\n",
    "    # Use glob to find all Excel files in the folder\n",
    "    excel_files = glob.glob(os.path.join(folder_path, \"*.xlsx\"))\n",
    "\n",
    "    # Initialize an empty list to store the DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through each Excel file and read it into a DataFrame\n",
    "    for excel_file in excel_files:\n",
    "        df = pd.read_excel(excel_file)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate the list of DataFrames into one DataFrame\n",
    "    all_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Filter the DataFrame based on the Filename column and the keyword\n",
    "    filtered_df = all_data[all_data['Filename'].str.contains(filename_search)]\n",
    "\n",
    "    # Initialize variables to store the extracted values\n",
    "    duration_hours = None\n",
    "    ctemp_avg_C = None\n",
    "    i0_avg = None\n",
    "    bstop_avg = None\n",
    "\n",
    "    # Check if there are matching rows\n",
    "    if not filtered_df.empty:\n",
    "        # Extract the values from the first matching row (assuming there's only one)\n",
    "        row = filtered_df.iloc[0]\n",
    "        duration_hours = row['duration_realtime_hr']\n",
    "        ctemp_avg_C = row['ctemp']\n",
    "        i0_avg = row['i0']\n",
    "        bstop_avg = row['bstop']\n",
    "\n",
    "    # Return the extracted values as a tuple\n",
    "    return duration_hours, ctemp_avg_C, i0_avg, bstop_avg\n",
    "\n",
    "######################### \n",
    "import os\n",
    "import re\n",
    "\n",
    "def search_dat_files_with_keywords(folder_path, keyword, bkg_keyword):\n",
    "    # Create a regular expression pattern to match the keyword and bkg_keyword in the filename\n",
    "    pattern = re.compile(f\".*{re.escape(bkg_keyword)}.*{re.escape(keyword)}.*\", re.IGNORECASE)\n",
    "\n",
    "    matching_dat_files = []\n",
    "\n",
    "    # List files in the folder (not including subfolders)\n",
    "    for file in os.listdir(folder_path):\n",
    "        if pattern.match(file) and file.endswith('.dat'):\n",
    "            matching_dat_files.append(file)\n",
    "\n",
    "    # Sort the matching files by modification time\n",
    "    matching_dat_files.sort(key=lambda file: os.path.getmtime(os.path.join(folder_path, file)))\n",
    "\n",
    "    return matching_dat_files\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "def process_sample_data(filename_search, folder_path, inst_parameters):\n",
    "    Sample_data = read_single_file_with_keyword(folder_path, filename_search)\n",
    "    q = Sample_data[\"q_nm^-1\"]\n",
    "    I_raw = Sample_data[\"I\"]\n",
    "    I_sigma = Sample_data[\"sigma\"]\n",
    "    I_norm = Sample_data[\"I\"] / inst_parameters[3]\n",
    "    I_norm_sigma = I_sigma / inst_parameters[3]           # sigma of normalised intensity\n",
    "    return q, I_raw, I_sigma, I_norm, I_norm_sigma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0927ab60",
   "metadata": {},
   "source": [
    "# SAXS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd13f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_saxs_base = '/Users/akmaurya/Library/CloudStorage/OneDrive-SLACNationalAcceleratorLaboratory/My Onedrive/Data_01/Methanolysis/Dec2023/OneD_integrated_SAXS_01_mask/Reduction'\n",
    "save_SAXS_norm_files = os.path.join(folder_path_saxs_base.replace(\"Reduction\", \"\"), \"Correction/Normlised\")\n",
    "#print (save_WAXS_norm_files)\n",
    "\n",
    "Run_number_search = \"Run9\"\n",
    "\n",
    "Run_number=Run_number_search  \n",
    "#Run_number = \"Run4\"\n",
    "folder_path = os.path.join(folder_path_saxs_base, Run_number)\n",
    "\n",
    "matching_files = search_dat_files_with_keywords(folder_path, \"\", Run_number_search)\n",
    "#matching_files = [\"sone_example.txt\", \"another_file.txt\", \"sone_another_file.txt\", \"file_without_prefix.txt\"]\n",
    "#print(modified_files)\n",
    "\n",
    "#for filename_search in matching_files:\n",
    "    #print(filename_search)\n",
    "\n",
    "\n",
    "for filename_search in matching_files:\n",
    "    #print(filename_search)\n",
    "    if \"dx_minus_1_SAXS\" in filename_search:\n",
    "        inst_filename = re.sub(r'(ctr\\d+).*$', r'\\1', filename_search)\n",
    "\n",
    "        # Remove \"sone_\" from the beginning of inst_filename\n",
    "        inst_filename = re.sub(r'^sone_', '', inst_filename)\n",
    "\n",
    "        #print(filename_search)\n",
    "        #print(inst_filename)\n",
    "        inst_parameters = extract_data_from_folder(folder_path, inst_filename)\n",
    "        q, I_raw_minus1, I_sigma_minus1, I_norm_minus1, I_norm_sigma_minus1 = process_sample_data(filename_search, folder_path, inst_parameters)\n",
    "\n",
    "    elif \"_dx0_SAXS\" in filename_search or \"_dx1_SAXS\" in filename_search:\n",
    "        inst_filename = re.sub(r'(ctr\\d+).*$', r'\\1', filename_search)\n",
    "        inst_filename = re.sub(r'^sone_', '', inst_filename)\n",
    "        #print(filename_search)\n",
    "        #print(inst_filename)\n",
    "        inst_parameters = extract_data_from_folder(folder_path, inst_filename)\n",
    "        q, I_raw_dx, I_sigma_dx, I_norm_dx, I_norm_sigma_dx = process_sample_data(filename_search, folder_path, inst_parameters)\n",
    "\n",
    "    elif \"_all_SAXS\" in filename_search:\n",
    "        print(filename_search)\n",
    "        inst_filename = re.sub(r'(ctr\\d+).*$', r'\\1_scan1_all_average', filename_search)\n",
    "        inst_parameters = extract_data_from_folder(folder_path, inst_filename)\n",
    "        #print(inst_parameters)\n",
    "        #print(inst_filename)\n",
    "        #print(filename_search)\n",
    "        #print(folder_path)\n",
    "        q, I_raw, I_sigma, I_norm, I_norm_sigma = process_sample_data(filename_search, folder_path, inst_parameters)\n",
    "\n",
    "\n",
    "        '''\n",
    "        #import matplotlib.pyplot as plt\n",
    "        # Create subplots with 'num_subplots' rows\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8,3))\n",
    "\n",
    "        # Plot data for the first subplot\n",
    "        \n",
    "        axs[0].loglog(q, I_raw, 'k-',label=\"Raw\")\n",
    "        axs[0].set_xlabel(\"q_nm^-1\")\n",
    "        axs[0].set_ylabel(\"I_avg_raw\")\n",
    "        axs[0].legend()\n",
    "        #axs[0].set_title(f\"raw Data Plot - {filename_search}\")\n",
    "\n",
    "        # Plot data for the second subplot\n",
    "        # Assuming you have different data for the second subplot, replace 'q_2', 'I_norm_2', 'I_raw_2', etc., with your actual data\n",
    "        axs[1].loglog(q, I_norm, 'r-',label=\"Normalized\")\n",
    "        \n",
    "        axs[1].set_xlabel(\"q_nm^-1\")\n",
    "        axs[1].set_ylabel(\"I_avg_normalized\")\n",
    "        axs[1].legend()\n",
    "        #axs[1].set_title(f\"Normalized Data Plot - {filename_search}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        '''\n",
    "\n",
    "        # Create a DataFrame for subtracted data\n",
    "        data = {\"q_nm^-1\": q, \"I_avg_nomrlised\": I_norm, \"I_norm_sigma\": I_norm_sigma}\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        dat_filename = f\"{save_SAXS_norm_files}/{inst_filename}_Norm.dat\"\n",
    "\n",
    "        #print(dat_filename)\n",
    "        headers = [\n",
    "            f\"filename_{filename_search}\",\n",
    "            f\"instrument_filename_{inst_filename}\",\n",
    "            'Time_duration_hr  ----- CTEMP -----  I0 ----- bstop',\n",
    "            f\"{inst_parameters}\", \"q_nm^-1 ------ I_avg_nomrlised ------ I_norm_sigma   \"\n",
    "        ]\n",
    "        commented_headers = ['# ' + header for header in headers]\n",
    "\n",
    "        with open(dat_filename, 'w') as dat_file:\n",
    "            dat_file.write('\\n'.join(commented_headers) + '\\n')\n",
    "            df.to_csv(dat_file, sep='\\t', index=False, header=False)\n",
    "\n",
    "    else:\n",
    "        print(\"No match found for sample files\")\n",
    "\n",
    "    #I_dx_minus_1_norm, I_dx0_norm, I_dx1_norm, I_avg_norm = [], [], [], []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
