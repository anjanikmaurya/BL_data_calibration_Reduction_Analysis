{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5be27eec",
   "metadata": {},
   "source": [
    "# BL 1-5: SAXS data correction and reduction\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65355923",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Created on Fri Apr 21 23:41:02 2023\n",
    "\n",
    "@author: akmaurya\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebdc2b8d",
   "metadata": {},
   "source": [
    "Global stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bcd328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from axis_plot_prop import axis_plot_prop \n",
    "import pyFAI\n",
    "import shutil\n",
    "import fabio\n",
    "from pyFAI.gui.jupyter.calib import Calibration\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89935775",
   "metadata": {},
   "source": [
    "# Common functions for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot setting fucntion\n",
    "\n",
    "def axis_plot_prop(ax):\n",
    "    \"\"\"\n",
    "    Set axis properties for plotting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib axis object\n",
    "        The axis object to set properties for.\n",
    "    \"\"\"\n",
    "\n",
    "    co = [\n",
    "        [0, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [0.44, 0.00, 0.99],\n",
    "        [1.00, 0.50, 0.10],\n",
    "        [0.75, 0.00, 0.75],\n",
    "        [0.50, 0.50, 0.50],\n",
    "        [0.50, 0.57, 0.00],\n",
    "        [0.64, 0.08, 0.18],\n",
    "        [0.93, 0.00, 0.00]\n",
    "    ]\n",
    "\n",
    "    plt.rcParams['axes.prop_cycle'] = plt.cycler(color=co)\n",
    "\n",
    "    ax.set_facecolor('white')\n",
    "    ax.spines['top'].set_linewidth(1)\n",
    "    ax.spines['right'].set_linewidth(1)\n",
    "    ax.spines['bottom'].set_linewidth(1)\n",
    "    ax.spines['left'].set_linewidth(1)\n",
    "    ax.tick_params(axis='both', which='both', direction='in', length=4, width=1)\n",
    "    ax.minorticks_on()\n",
    "    ax.set_xlabel('X Label', fontsize=20, fontname='Arial')\n",
    "    ax.set_ylabel('Y Label', fontsize=20, fontname='Arial')\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "    ax.legend(frameon=False)\n",
    "    #ax.set_box_aspect(1)\n",
    "    plt.tight_layout()\n",
    "#2D plot settings function\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def set_pcolor_properties():\n",
    "    \"\"\"\n",
    "    Sets the best figure properties for pcolormap plot\n",
    "    \"\"\"\n",
    "    plt.rcParams[\"figure.figsize\"] = (6, 6)  # Set the figure size to 8 inches by 6 inches\n",
    "    plt.rcParams[\"font.size\"] = 20  # Set the font size to 12 points\n",
    "    plt.rcParams[\"axes.linewidth\"] = 1.5  # Set the linewidth of the axes to 1.5\n",
    "    #plt.rcParams[\"axes.grid\"] = True  # Show the grid\n",
    "    #plt.rcParams[\"grid.alpha\"] = 0.5  # Set the transparency of the grid to 0.5\n",
    "    #plt.rcParams[\"grid.linewidth\"] = 0.5  # Set the linewidth of the grid to 0.5\n",
    "    plt.rcParams[\"image.cmap\"] = \"viridis\"  # Set the colormap to viridis\n",
    "    plt.rcParams[\"image.interpolation\"] = \"nearest\"  # Set the interpolation to nearest\n",
    "\n",
    "# create and update folder to save data\n",
    "def create_update_subfolder(base_folder, common_keyword):\n",
    "    common_folder_path = os.path.join(base_folder, common_keyword)\n",
    "    \n",
    "    if os.path.exists(base_folder) and os.path.isdir(common_folder_path):\n",
    "        print(f\"Subfolder '{common_keyword}' already exists within the base folder. Doing nothing.\")\n",
    "    else:\n",
    "        os.makedirs(common_folder_path)\n",
    "        print(f\"Subfolder '{common_keyword}' created within the base folder.\")\n",
    "\n",
    "    return common_folder_path\n",
    "\n",
    "# Convert pixel by pixel to qx by qy 2D SAXS plot\n",
    "\n",
    "def convert_saxs_to_q(data, pixel_size, wavelength, detector_distance, beam_x, beam_y):\n",
    "    # Define grid of pixel coordinates (assumes data is square)\n",
    "    #x_pixels,y_pixels = data.shape\n",
    "    x_pixels = 981\n",
    "    y_pixels = 1043\n",
    "    x_coords = np.arange(x_pixels) - beam_x\n",
    "    y_coords = np.arange(y_pixels) - beam_y\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    # Convert to q-space coordinates\n",
    "    qx = 1e-9*2 * np.pi / wavelength * np.sin(pixel_size * xx / detector_distance)\n",
    "    qy = 1e-9*2 * np.pi / wavelength * np.sin(pixel_size * yy / detector_distance)\n",
    "\n",
    "    #print (len(xx))\n",
    "    \n",
    "    return qx, qy\n",
    "\n",
    "\n",
    "    #print (qx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edf2e740",
   "metadata": {},
   "source": [
    "# input varibales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f16c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "#poni files saxs and waxs\n",
    "folder_path_poni_mask = '/Users/akmaurya/Library/CloudStorage/OneDrive-SLACNationalAcceleratorLaboratory/My Onedrive/Data_01/Methanolysis/Python/Data_Reduction/Convert_and_calibrate'\n",
    "# Set the directory containing the raw and pdi files\n",
    "raw_dir = '/Users/akmaurya/Library/CloudStorage/OneDrive-SLACNationalAcceleratorLaboratory/My Onedrive/Data_01/Methanolysis/Dec2023'\n",
    "SAXS_poni_file = \"Methanolysis_Dec2023_SAXS.poni\"\n",
    "WAXS_poni_file = \"Methanolysis_Dec2023_WAXS_GUI.poni\"\n",
    "\n",
    "\n",
    "######################### to be change for every Run ########################\n",
    "keyword=\"Run1*dx0*\"\n",
    "\n",
    "common_keyword = \"Run1\" \n",
    "\n",
    "##################################################################################\n",
    "\n",
    "# Base folders\n",
    "base_SAXS_folder = \"OneD_integrated_SAXS_01_mask/Reduction\"\n",
    "base_SAXS_folder = os.path.join(raw_dir, base_SAXS_folder)\n",
    "\n",
    "base_WAXS_folder = \"OneD_integrated_WAXS_01/Reduction\"\n",
    "base_WAXS_folder = os.path.join(raw_dir, base_WAXS_folder)\n",
    "# Create or update folders and get updated folder paths\n",
    "SAXS_folder_name = create_update_subfolder(base_SAXS_folder, common_keyword)\n",
    "WAXS_folder_name = create_update_subfolder(base_WAXS_folder, common_keyword)\n",
    "\n",
    "print(SAXS_folder_name)\n",
    "\n",
    "# Use SAXS_folder_name and WAXS_folder_name as needed in your code\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e998751b",
   "metadata": {},
   "source": [
    "# Read poni file for azimuthal integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbd91d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "poni_file_path = os.path.join(folder_path_poni_mask,SAXS_poni_file)\n",
    "ai = pyFAI.load(poni_file_path)\n",
    "\n",
    "print(ai)\n",
    "\n",
    "#Define detector parameters for 2D q conversion\n",
    "pixel_size = ai.get_pixel1() # in meters \n",
    "wavelength = ai.get_wavelength() #1.54e-10  # in meters\n",
    "detector_distance = ai.get_dist()  # in meters\n",
    "beam_x= 491.641\n",
    "beam_y= 627.426 # beam center x-coordinate in pixels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53af291f",
   "metadata": {},
   "source": [
    "# Set the directory containing the raw and pdi files and list the keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a7223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "\n",
    "def find_folders_with_keyword(folder_path, keyword, skip_folders=None):\n",
    "    folder_names = []\n",
    "    for item in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isdir(item_path) and fnmatch.fnmatch(item, keyword):\n",
    "            if skip_folders is None or item not in skip_folders:\n",
    "                folder_names.append(item)\n",
    "\n",
    "    return folder_names\n",
    "\n",
    "\n",
    "files = os.listdir(raw_dir)\n",
    "\n",
    "# Input folder path and keyword\n",
    "folder_path = raw_dir\n",
    "skip_folders = ['OneD_integrated_SAXS_01_Norm']  # Replace with folder names to skip, or set to None to skip no folders\n",
    "\n",
    "# Find folder names containing the keyword, skipping specified folders\n",
    "folder_names_with_keyword = find_folders_with_keyword(folder_path, keyword, skip_folders)\n",
    "\n",
    "# Extract \"ctr\" values from folder names\n",
    "ctr_values = [int(name.split('_ctr')[-1]) for name in folder_names_with_keyword]\n",
    "\n",
    "# Sort folder names based on \"ctr\" values\n",
    "sorted_folder_names = [x for _, x in sorted(zip(ctr_values, folder_names_with_keyword))]\n",
    "\n",
    "# Replace \"dx0\" with \"*\"\n",
    "sorted_folder_names = [name.replace(\"dx0\", \"*\") for name in sorted_folder_names]\n",
    "\n",
    "keywords=(sorted_folder_names)\n",
    "\n",
    "# Print the sorted folder names\n",
    "print(\"keywords = \")\n",
    "for sorted_name in sorted_folder_names:\n",
    "    print(f\"    '{sorted_name}',\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdaa0da0",
   "metadata": {},
   "source": [
    "# Data reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e180c7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re   \n",
    "\n",
    "# Define the base folder name and create the folder\n",
    "\n",
    "folder_path = SAXS_folder_name\n",
    "print (folder_path)\n",
    "\n",
    "filename_dx_1 = None \n",
    "filename_dx0 = None\n",
    "filename_dx1 = None\n",
    "\n",
    "for keyword in keywords:\n",
    "    # Create a list of file paths that match the file types and current keyword\n",
    "    raw_file_pattern = f'*{keyword}/SAXS/*.raw'\n",
    "    print (raw_file_pattern)\n",
    "    raw_file_paths = sorted(glob.glob(os.path.join(raw_dir, raw_file_pattern)))\n",
    "\n",
    "    # Initialize the accumulators for the averaged image and pdi data\n",
    "    avg_image = np.zeros((1043, 981))\n",
    "    # Initialize variables to store averages\n",
    "    avg_dx_minus_1 = np.zeros((1043, 981), dtype=np.int32)\n",
    "    avg_dx_0 = np.zeros((1043, 981), dtype=np.int32)\n",
    "    avg_dx_1 = np.zeros((1043, 981), dtype=np.int32)\n",
    "\n",
    "    for i, raw_file_path in enumerate(raw_file_paths):\n",
    "        # Read the raw file\n",
    "        data = np.fromfile(raw_file_path, dtype=np.int32).reshape(1043, 981)\n",
    "        avg_image += data\n",
    "\n",
    "        file_name = os.path.splitext(os.path.basename(raw_file_path))[0]\n",
    "\n",
    "        if \"_dx-1_\" in file_name:\n",
    "            #print(file_name)\n",
    "            filename_dx_1 = file_name\n",
    "            filename_dx_1 = filename_dx_1[5:-10]\n",
    "            #print(i)\n",
    "            avg_dx_minus_1 += data\n",
    "        elif \"_dx0_\" in file_name:\n",
    "            avg_dx_0 += data\n",
    "            #print(file_name)\n",
    "            filename_dx0 = file_name\n",
    "            filename_dx0 = filename_dx0[5:-10]\n",
    "        elif \"_dx1_\" in file_name:\n",
    "            avg_dx_1 += data\n",
    "            filename_dx1 = file_name\n",
    "            filename_dx1 = filename_dx1[5:-10]\n",
    "            #print(file_name)\n",
    "        else: \n",
    "            continue   \n",
    "        \n",
    "         # Calculate the final averages\n",
    "    dx_1_size=len([f for f in raw_file_paths if \"_dx-1_\" in f])\n",
    "    #print(dx_1_size)\n",
    "    avg_dx_minus_1 = (avg_dx_minus_1 / dx_1_size)\n",
    "    dx0_size = len([f for f in raw_file_paths if \"_dx0_\" in f])\n",
    "    avg_dx_0 = (avg_dx_0 / dx0_size)\n",
    "    dx1_size=len([f for f in raw_file_paths if \"_dx1_\" in f])\n",
    "    avg_dx_1 = (avg_dx_1 /dx1_size)\n",
    "\n",
    "    avg_image /= len(raw_file_paths)\n",
    "\n",
    "    # Assuming raw_file_paths[-1] contains the file path\n",
    "    file_name, file_extension = os.path.splitext(os.path.basename(raw_file_paths[-1]))\n",
    "\n",
    "    # Remove the last 10 characters from the file name and the \".raw\" extension\n",
    "    file_name = file_name[5:-10]\n",
    "    file_extension = \"\"\n",
    "\n",
    "    avg_file_name = f\"{file_name}{len(raw_file_paths)}average{file_extension}\"\n",
    "    #print(avg_file_name)\n",
    "    #print (avg_file_name)\n",
    "\n",
    "    # Create the modified filenames\n",
    "    avg_dx_1_filename = f\"{filename_dx_1}_{dx_1_size}average\"\n",
    "    #print (avg_dx_1_filename)\n",
    "    avg_dx0_filename = f\"{filename_dx0}_{dx0_size}average\"\n",
    "    avg_dx1_filename = f\"{filename_dx1}_{dx1_size}average\"\n",
    "    \n",
    "    \n",
    "    #print (f\"{keyword}_{num_raw_files}_average\")\n",
    "    set_pcolor_properties()\n",
    "    fign, axsn = plt.subplots(figsize=(4,4))\n",
    "    qx,qy=convert_saxs_to_q(avg_image, pixel_size, wavelength, detector_distance, beam_x, beam_y)\n",
    "    #im = axsn.pcolormesh(qx,qy,np.log10(avg_image), cmap='jet')\n",
    "    im = axsn.pcolormesh(qx, qy, np.log10(avg_image), cmap='jet', vmin=-0.5, vmax=1.5)\n",
    "    axsn.set_aspect('equal')\n",
    "    cbar=fign.colorbar(im, ax=axsn, shrink=0.8, label='Intensity [a.u.]')\n",
    "\n",
    "\n",
    "    #fign.colorbar(im, ax=axsn)\n",
    "    axsn.set_xlabel(r\"$q_x$ [nm$^{-1}$]\")\n",
    "    axsn.set_ylabel(r\"$q_y$ [nm$^{-1}$]\")\n",
    "    axsn.set_title(f\"{avg_file_name}\",fontsize=14,y=1.05)\n",
    "    # set tick levels\n",
    "    axsn.set_xticks([-2, -1, 0, 1, 2])\n",
    "    axsn.set_yticks([-2.4, -1.6, -0.8, 0.0, 0.8, 1.6])\n",
    "    axsn.tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    #file_path = os.path.join(folder_path, f\"_{avg_file_name}.png\")\n",
    "    #plt.savefig(file_path, dpi=300, bbox_inches='tight', format='png')\n",
    "    plt.show()\n",
    "    \n",
    " \n",
    "    # Create a figure with a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "    # Plot avg_image in the top-left subplot\n",
    "    qx_avg, qy_avg = convert_saxs_to_q(avg_image, pixel_size, wavelength, detector_distance, beam_x, beam_y)\n",
    "    im_avg = axs[0, 0].pcolormesh(qx_avg, qy_avg, np.log10(avg_image), cmap='jet', vmin=-0.5, vmax=1.5)\n",
    "    axs[0, 0].set_aspect('equal')\n",
    "    cbar_avg = fig.colorbar(im_avg, ax=axs[0, 0], shrink=0.65, label='Intensity [a.u.]')\n",
    "    axs[0, 0].set_xlabel(r\"$q_x$ [nm$^{-1}$]\")\n",
    "    axs[0, 0].set_ylabel(r\"$q_y$ [nm$^{-1}$]\")\n",
    "    axs[0, 0].set_title(f\"{avg_file_name}\", fontsize=14, y=1.05)\n",
    "    axs[0, 0].set_xticks([-2, -1, 0, 1, 2])\n",
    "    axs[0, 0].set_yticks([-2.4, -1.6, -0.8, 0.0, 0.8, 1.6])\n",
    "    axs[0, 0].tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    # Plot avg_dx_minus_1 in the top-right subplot\n",
    "    qx_minus_1, qy_minus_1 = convert_saxs_to_q(avg_dx_minus_1, pixel_size, wavelength, detector_distance, beam_x, beam_y)\n",
    "    im_minus_1 = axs[0, 1].pcolormesh(qx_minus_1, qy_minus_1, np.log10(avg_dx_minus_1), cmap='jet', vmin=-0.5, vmax=1.5)\n",
    "    axs[0, 1].set_aspect('equal')\n",
    "    cbar_minus_1 = fig.colorbar(im_minus_1, ax=axs[0, 1], shrink=0.65, label='Intensity [a.u.]')\n",
    "    axs[0, 1].set_xlabel(r\"$q_x$ [nm$^{-1}$]\")\n",
    "    axs[0, 1].set_ylabel(r\"$q_y$ [nm$^{-1}$]\")\n",
    "    axs[0, 1].set_title(\"avg_dx_minus_1\", fontsize=14, y=1.05)\n",
    "    axs[0, 1].set_xticks([-2, -1, 0, 1, 2])\n",
    "    axs[0, 1].set_yticks([-2.4, -1.6, -0.8, 0.0, 0.8, 1.6])\n",
    "    axs[0, 1].tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    # Plot avg_dx_0 in the bottom-left subplot\n",
    "    qx_0, qy_0 = convert_saxs_to_q(avg_dx_0, pixel_size, wavelength, detector_distance, beam_x, beam_y)\n",
    "    im_0 = axs[1, 0].pcolormesh(qx_0, qy_0, np.log10(avg_dx_0), cmap='jet', vmin=-0.5, vmax=1.5)\n",
    "    axs[1, 0].set_aspect('equal')\n",
    "    cbar_0 = fig.colorbar(im_0, ax=axs[1, 0], shrink=0.65, label='Intensity [a.u.]')\n",
    "    axs[1, 0].set_xlabel(r\"$q_x$ [nm$^{-1}$]\")\n",
    "    axs[1, 0].set_ylabel(r\"$q_y$ [nm$^{-1}$]\")\n",
    "    axs[1, 0].set_title(\"avg_dx_0\", fontsize=14, y=1.05)\n",
    "    axs[1, 0].set_xticks([-2, -1, 0, 1, 2])\n",
    "    axs[1, 0].set_yticks([-2.4, -1.6, -0.8, 0.0, 0.8, 1.6])\n",
    "    axs[1, 0].tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    # Plot avg_dx_1 in the bottom-right subplot\n",
    "    qx_1, qy_1 = convert_saxs_to_q(avg_dx_1, pixel_size, wavelength, detector_distance, beam_x, beam_y)\n",
    "    im_1 = axs[1, 1].pcolormesh(qx_1, qy_1, np.log10(avg_dx_1), cmap='jet', vmin=-0.5, vmax=1.5)\n",
    "    axs[1, 1].set_aspect('equal')\n",
    "    cbar_1 = fig.colorbar(im_1, ax=axs[1, 1], shrink=0.65, label='Intensity [a.u.]')\n",
    "    axs[1, 1].set_xlabel(r\"$q_x$ [nm$^{-1}$]\")\n",
    "    axs[1, 1].set_ylabel(r\"$q_y$ [nm$^{-1}$]\")\n",
    "    axs[1, 1].set_title(\"avg_dx_1\", fontsize=14, y=1.05)\n",
    "    axs[1, 1].set_xticks([-2, -1, 0, 1, 2])\n",
    "    axs[1, 1].set_yticks([-2.4, -1.6, -0.8, 0.0, 0.8, 1.6])\n",
    "    axs[1, 1].tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #file_path = os.path.join(folder_path, f\"_{avg_file_name}_all.png\")\n",
    "    #plt.savefig(file_path, dpi=50, bbox_inches='tight', format='png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "   # Create a figure with 1 row and 2 columns\n",
    "    fig, axs = plt.subplots(2,1, figsize=(6, 12))\n",
    "\n",
    "    for j, raw_file_path in enumerate(raw_file_paths):\n",
    "        data = np.fromfile(raw_file_path, dtype=np.int32).reshape(1043, 981)\n",
    "        file_name = os.path.splitext(os.path.basename(raw_file_path))[0]\n",
    "\n",
    "        q, I, error = ai.integrate1d(data, 1000, error_model='poisson')#, filename=os.path.join(folder_path, f\"{file_name}_SAXS.dat\"))\n",
    "        axs[0].loglog(q, I, label=file_name)\n",
    "        #axs[0].set_xlabel(\"q (nm$^{-1}$)\")\n",
    "        axs[0].set_ylabel(\"Intensity (a.u.)\")\n",
    "        axs[0].set_xlim([0.05, 3])\n",
    "        axs[0].legend(fontsize=12)\n",
    "        axs[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left',fontsize=12)\n",
    "\n",
    "    # Integrate and plot for avg_dx_minus_1\n",
    "    q_dx_minus_1, I_dx_minus_1,error_dx_minus_1 = ai.integrate1d(avg_dx_minus_1, 1000, error_model='poisson', filename=os.path.join(folder_path, f\"{avg_dx_1_filename}_dx_minus_1_SAXS.dat\"))\n",
    "    axs[1].loglog(q_dx_minus_1, I_dx_minus_1, '-g',label=f\"{avg_dx_1_filename}_dx_minus_1\")\n",
    "    axs[1].set_xlabel(\"q (nm$^{-1}$)\")\n",
    "    axs[1].set_ylabel(\"Intensity (a.u.)\")\n",
    "    axs[1].set_xlim([0.05, 3])\n",
    "    axs[1].legend(fontsize=12)\n",
    "\n",
    "    # Integrate and plot for avg_dx_0\n",
    "    q_dx_0, I_dx_0, error_dx_0 = ai.integrate1d(avg_dx_0, 1000,  error_model='poisson',filename=os.path.join(folder_path, f\"{avg_dx0_filename}_dx0_SAXS.dat\"))\n",
    "    axs[1].loglog(q_dx_0, I_dx_0, '-b',label=f\"{avg_dx0_filename}_dx0\")\n",
    "    axs[1].set_xlabel(\"q (nm$^{-1}$)\")\n",
    "    axs[1].set_ylabel(\"Intensity (a.u.)\")\n",
    "    axs[1].set_xlim([0.05, 3])\n",
    "    axs[1].legend(fontsize=12)\n",
    "\n",
    "    # Integrate and plot for avg_dx_1\n",
    "    q_dx_1, I_dx_1, error_dx_1 = ai.integrate1d(avg_dx_1, 1000,  error_model='poisson', filename=os.path.join(folder_path, f\"{avg_dx1_filename}_dx1_SAXS.dat\"))\n",
    "    axs[1].loglog(q_dx_1, I_dx_1, '-k',label=f\"{avg_dx1_filename}_dx1\")\n",
    "    axs[1].set_xlabel(\"q (nm$^{-1}$)\")\n",
    "    axs[1].set_ylabel(\"Intensity (a.u.)\")\n",
    "    axs[1].set_xlim([0.05, 3])\n",
    "    axs[1].legend(fontsize=12)\n",
    "    axs[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left',fontsize=12)\n",
    "    \n",
    "    q, I, error_avg = ai.integrate1d(avg_image, 1000,error_model='poisson', filename=os.path.join(folder_path, f\"{avg_file_name}_all_SAXS.dat\"))\n",
    "    axs[1].loglog(q, I, '-r',label=f\"{avg_file_name}_all\")\n",
    "    axs[1].set_xlabel(\"q (nm$^{-1}$)\")\n",
    "    axs[1].set_ylabel(\"Intensity (a.u.)\")\n",
    "    axs[1].set_xlim([0.05, 3])\n",
    "    axs[1].legend(fontsize=12)\n",
    "    axs[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left',fontsize=12)\n",
    "\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    filename_dx_1 = None \n",
    "    filename_dx0 = None\n",
    "    filename_dx1 = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecb3044c",
   "metadata": {},
   "source": [
    "# read parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import pickle  # Import the pickle module\n",
    "\n",
    "def process_and_store_selected_columns(raw_dir, keywords):\n",
    "    # Create an empty dictionary to store the selected columns for each file\n",
    "    selected_columns_dict = {}\n",
    "\n",
    "    for keyword in keywords:\n",
    "        # Create a list of file paths that match the file types and current keyword\n",
    "        #raw_file_pattern = f'*{keyword}/SAXS/*.raw'\n",
    "        pdi_file_pattern = f'*{keyword}/*_scan1.csv'\n",
    "        #raw_file_paths = sorted(glob.glob(os.path.join(raw_dir, raw_file_pattern)))\n",
    "        pdi_file_paths = sorted(glob.glob(os.path.join(raw_dir, pdi_file_pattern)))\n",
    "\n",
    "        for pdi_file_path in pdi_file_paths:\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(pdi_file_path)\n",
    "                file_name = os.path.splitext(os.path.basename(pdi_file_path))[0]\n",
    "                #print(f\"Reading file: {file_name}\")\n",
    "\n",
    "                # Extract columns 4, 7, 9, and 15\n",
    "                selected_columns = df.iloc[:, [2, 3, 6, 10, 29, 30]]\n",
    "\n",
    "                # Store the selected columns in the dictionary with the file name as the key\n",
    "                selected_columns_dict[file_name] = selected_columns\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {pdi_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return selected_columns_dict\n",
    "\n",
    "# Call the function and pass the raw_dir and keywords as arguments\n",
    "#raw_dir = 'path_to_your_raw_files_directory'\n",
    "#keywords = ['keyword1', 'keyword2', 'keyword3']  # Replace with your actual keywords\n",
    "result_dict = process_and_store_selected_columns(raw_dir, keywords)\n",
    "\n",
    "# Save the result_dict to a file using pickle\n",
    "output_dir = folder_path  # Replace with your desired output directory\n",
    "output_file = os.path.join(output_dir, 'result_dict.pkl')\n",
    "\n",
    "try:\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(result_dict, f)\n",
    "    print(f\"Result_dict saved to {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the result_dict: {e}\")\n",
    "\n",
    "# Now, result_dict contains the selected columns for each file\n",
    "# You can access the data by using keys (file names) in the result_dict.\n",
    "\n",
    "# To load the result_dict back from the file, you can use the following code:\n",
    "with open(output_file, 'rb') as f:\n",
    "     result_dict = pickle.load(f)\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4252a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create lists to store averaged values\n",
    "bstop_dx_avg = []\n",
    "ctemp_dx_avg = []\n",
    "timer_dx_avg = []\n",
    "i0_dx_avg = []\n",
    "\n",
    "bstop_avg_all = []\n",
    "ctemp_avg_all = []\n",
    "timer_avg_all = []\n",
    "i0_avg_all = []\n",
    "\n",
    "avg_file_name = []\n",
    "\n",
    "# Call the function and pass the raw_dir and keywords as arguments\n",
    "for keyword in keywords:\n",
    "    \n",
    "    i0_dx_avg_tmp=[]\n",
    "    bstop_dx_avg_tmp=[]\n",
    "    ctemp_dx_avg_tmp=[]\n",
    "    timer_dx_avg_tmp=[]\n",
    "    \n",
    "    # Create a list of file paths that match the file types and current keyword\n",
    "    #raw_file_pattern = f'*{keyword}/SAXS/*.raw'\n",
    "    pdi_file_pattern = f'*{keyword}/*_scan1.csv'\n",
    "    #raw_file_paths = sorted(glob.glob(os.path.join(raw_dir, raw_file_pattern)))\n",
    "    pdi_file_paths = sorted(glob.glob(os.path.join(raw_dir, pdi_file_pattern)))\n",
    "    \n",
    "    for pdi_file_path in pdi_file_paths:\n",
    "        file_name = os.path.splitext(os.path.basename(pdi_file_path))[0]\n",
    "        #print (file_name)\n",
    "        \n",
    "        # Let's say you want to read individual columns from a file named \"example_file\"\n",
    "\n",
    "        # Access the selected columns for the specified file\n",
    "        selected_columns = result_dict.get(file_name)\n",
    "\n",
    "        if selected_columns is not None:\n",
    "            # Access individual columns by index or name\n",
    "            i0 = selected_columns.iloc[:, 1]\n",
    "            bstop = selected_columns.iloc[:, 2]  # Access the first column (index 2)\n",
    "            ctemp = selected_columns.iloc[:, 4]  # Access the second column (index 4)\n",
    "            timer = selected_columns.iloc[:, 5]  # Access the third column (index 5)\n",
    "            #print (bstop)\n",
    "            try:\n",
    "                # Convert the columns to numeric values, skipping non-numeric data\n",
    "                i0 = pd.to_numeric(i0, errors='coerce')\n",
    "                bstop = pd.to_numeric(bstop, errors='coerce')\n",
    "                ctemp = pd.to_numeric(ctemp, errors='coerce')\n",
    "                timer = pd.to_numeric(timer, errors='coerce')\n",
    "                \n",
    "                #print (bstop)\n",
    "\n",
    "                # Calculate the average values, excluding NaN values\n",
    "                i0_average = i0.mean(skipna=True)\n",
    "                bstop_average = bstop.mean(skipna=True)\n",
    "                ctemp_average = ctemp.mean(skipna=True)\n",
    "                timer_average = timer.mean(skipna=True)\n",
    "\n",
    "                # Append the averages to the respective lists\n",
    "                i0_dx_avg_tmp.append(i0_average)\n",
    "                bstop_dx_avg_tmp.append(bstop_average)\n",
    "                ctemp_dx_avg_tmp.append(ctemp_average)\n",
    "                timer_dx_avg_tmp.append(timer_average)\n",
    "                \n",
    "                i0_dx_avg.append(i0_average)\n",
    "                bstop_dx_avg.append(bstop_average)\n",
    "                ctemp_dx_avg.append(ctemp_average)\n",
    "                timer_dx_avg.append(timer_average)\n",
    "                \n",
    "                \n",
    "                #print (f'{file_name}={ctemp_average}')\n",
    "            except ValueError as e:\n",
    "                print(f\"Error processing file '{file_name}': {str(e)}\")\n",
    "        else:\n",
    "            print(f\"File '{file_name}' not found in the selected_columns_dict.\")\n",
    "    #print (file_name)\n",
    "    \n",
    "    avg_file_name_tmp = f'{file_name}_all_average'\n",
    "    avg_file_name.append(avg_file_name_tmp)\n",
    "    #print (avg_file_name)\n",
    "    # Calculate the average values, excluding NaN values\n",
    "    \n",
    "    i0_average_tmp = sum(i0_dx_avg_tmp) / len(i0_dx_avg_tmp)\n",
    "    bstop_average_tmp = sum(bstop_dx_avg_tmp) / len(bstop_dx_avg_tmp)\n",
    "    ctemp_average_tmp = sum(ctemp_dx_avg_tmp) / len(ctemp_dx_avg_tmp)\n",
    "    timer_average_tmp = sum(timer_dx_avg_tmp) / len(timer_dx_avg_tmp)\n",
    "    \n",
    "    # Append the averages to the respective lists\n",
    "    i0_avg_all.append(i0_average_tmp)\n",
    "    bstop_avg_all.append(bstop_average_tmp)\n",
    "    ctemp_avg_all.append(ctemp_average_tmp)\n",
    "    timer_avg_all.append(timer_average_tmp)\n",
    "    \n",
    "    #print (ctemp_average_tmp)\n",
    "    \n",
    "#print (ctemp_dx_avg)   \n",
    "    \n",
    "# Create a figure and axis for plotting\n",
    "#print (f'{ctemp_averages}')\n",
    "\n",
    "# Initialize the result list with the first value as 0\n",
    "time_durations = [0]\n",
    "duration_csv_avg_all = [0]\n",
    "\n",
    "# Subtract the first value from the rest and append to the result list\n",
    "for i in range(1, len(timer_dx_avg)):\n",
    "    duration = (timer_dx_avg[i] - timer_dx_avg[0])/3600\n",
    "    time_durations.append(duration)\n",
    "\n",
    "\n",
    "# Subtract the first value from the rest and append to the result list\n",
    "for i in range(1, len(timer_avg_all)):\n",
    "    duration_avg = (timer_avg_all[i] - timer_avg_all[0])/3600\n",
    "    duration_csv_avg_all.append(duration_avg)\n",
    "\n",
    "\n",
    "#print(f\"time={duration_avg_all}_ctemp_all={ctemp_avg_all}\\n\")\n",
    "# Now time_durations contains the time durations\n",
    "#print(time_durations)\n",
    "\n",
    "\n",
    "# Get the first item from keywords and use the first 2 characters as the file name prefix\n",
    "first_keyword = keywords[0]\n",
    "file_name_prefix = first_keyword[:4]\n",
    "\n",
    "# Plot the parameter values against Timer in the same plot\n",
    "fig3, axs3 = plt.subplots(figsize=(10, 6))\n",
    "axs3.set_facecolor('white')\n",
    "\n",
    "\n",
    "#axis_plot_prop(axs3)\n",
    "\n",
    "# Plot with marker size 10 and transparent legend\n",
    "#axs3.plot(time_duration, i0_values, '-o', markersize=10, label='i0')    \n",
    "#axs3.plot(time_durations, bstop_averages, 'o', markersize=10, label='bstop')\n",
    "#axs3.plot(time_duration, temp_values, '-<', markersize=10, label='TEMP')\n",
    "axs3.plot(time_durations, ctemp_dx_avg,'ok', markersize=10, label='CTEMP')\n",
    "\n",
    "axs3.plot(duration_csv_avg_all, ctemp_avg_all,'*r', markersize=10, label='CTEMP_avg')\n",
    "\n",
    "\n",
    "axs3.set_xlabel('Time Duration [Hrs]')\n",
    "axs3.set_ylabel('Temperature [Â°C]')\n",
    "#axs3.set_title('Parameter values vs Timer')\n",
    "\n",
    "#axs3.legend(framealpha=0.5)  # Set legend transparency\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to parse the date and time string\n",
    "def parse_datetime(datetime_str):\n",
    "    return datetime.strptime(datetime_str, \"%a %b %d %H:%M:%S %Y\")\n",
    "\n",
    "# Function to calculate duration in seconds\n",
    "def calculate_duration(start_time, end_time):\n",
    "    return (end_time - start_time).total_seconds()\n",
    "\n",
    "# Function to convert seconds to hours\n",
    "def seconds_to_hours(seconds):\n",
    "    return timedelta(seconds=seconds).total_seconds() / 3600.0\n",
    "\n",
    "# Initialize offset as None\n",
    "offset = None\n",
    "durations = []  # List to store durations in seconds\n",
    "real_times = []  # List to store real times\n",
    "file_names = []  # List to store file names\n",
    "durations_avg_all = []\n",
    "\n",
    "# Iterate through files in the folder\n",
    "for keyword in keywords:\n",
    "    # Create a list of file paths that match the file types and current keyword\n",
    "    spec_file_pattern = f'*{keyword}/{keyword}'\n",
    "    file_paths = sorted(glob.glob(os.path.join(raw_dir, spec_file_pattern)))\n",
    "    durations_tmp = []\n",
    "    for file_path in file_paths: \n",
    "        #file_name = os.path.splitext(os.path.basename(pdi_file_path))[0]\n",
    "        file_name = os.path.basename(file_path) \n",
    "        #print(file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            start_time = None\n",
    "            end_time = None\n",
    "\n",
    "            for line in lines:\n",
    "                if line.startswith(\"#D\"):\n",
    "                    if start_time is None:\n",
    "                        if offset is None:\n",
    "                            offset = parse_datetime(line[3:].strip())\n",
    "                        start_time = parse_datetime(line[3:].strip())\n",
    "                    end_time = parse_datetime(line[3:].strip())\n",
    "\n",
    "            if start_time and end_time:\n",
    "                duration_seconds = calculate_duration(offset, end_time)\n",
    "                duration_hours = seconds_to_hours(duration_seconds)\n",
    "                durations_tmp.append(duration_hours)\n",
    "                durations.append(duration_hours)# Store duration in hours\n",
    "                real_times.append(start_time)  # Store real time\n",
    "                file_name = os.path.basename(file_path)\n",
    "                file_names.append(file_name)\n",
    "            else:\n",
    "                print(f\"Start and end times not found in {file_path}\")\n",
    "    durations_avg_tmp = sum(durations_tmp)/len(durations_tmp)\n",
    "    durations_avg_all.append(durations_avg_tmp)\n",
    "#print(durations_avg_all)\n",
    "# Create a DataFrame to store the data\n",
    "data = {\n",
    "    \"Filename\": file_names,\n",
    "    \"Real Time\": real_times,\n",
    "    \"duration_realtime_hr\":durations,\n",
    "    \"time_duration_CSV\": time_durations,\n",
    "    \"ctemp\": ctemp_dx_avg,\n",
    "    \"i0\": i0_dx_avg,\n",
    "    \"bstop\": bstop_dx_avg\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Convert the data to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Get the first item from keywords and use the first 2 characters as the file name prefix\n",
    "first_keyword = keywords[0]\n",
    "file_name_prefix = first_keyword[:4]\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_folder = folder_path  # Replace with your desired folder path\n",
    "output_file_name = f'{file_name_prefix}_dx_time_temp_i0_bstop.xlsx'  # Replace with your desired file name\n",
    "output_file_path = os.path.join(output_folder, output_file_name)\n",
    "df.to_excel(output_file_path, index=False)\n",
    "#print (f 'dx average parameters saved to {output_file_path}')\n",
    "############################ Average\n",
    "data_avg = {\n",
    "    \"Filename\": avg_file_name,\n",
    "    \"duration_realtime_hr\":durations_avg_all,\n",
    "    \"ctemp\": ctemp_avg_all,\n",
    "    \"i0\": i0_avg_all,\n",
    "    \"bstop\": bstop_avg_all\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Convert the data to a pandas DataFrame\n",
    "df = pd.DataFrame(data_avg)\n",
    "\n",
    "# Get the first item from keywords and use the first 2 characters as the file name prefix\n",
    "first_keyword = keywords[0]\n",
    "file_name_prefix = first_keyword[:4]\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_folder = folder_path  # Replace with your desired folder path\n",
    "output_file_name = f'{file_name_prefix}_avg_all_time_temp_i0_bstop.xlsx'  # Replace with your desired file name\n",
    "output_file_path_avg = os.path.join(output_folder, output_file_name)\n",
    "df.to_excel(output_file_path_avg, index=False)\n",
    "\n",
    "\n",
    "# Plotting (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(file_names, durations)\n",
    "plt.ylabel(\"Duration (hours)\")\n",
    "plt.title(\"Duration vs. Filename\")\n",
    "plt.xlabel(\"#Filename\")\n",
    "plt.xticks([])  # Remove x-axis ticks\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'dx average parameters saved to {output_file_path}')\n",
    "print(f'all dx averaged parameters saved to {output_file_path_avg}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d35c9b9c",
   "metadata": {},
   "source": [
    "# WAXS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def set_pcolor_properties():\n",
    "    \"\"\"\n",
    "    Sets the best figure properties for pcolormap plot\n",
    "    \"\"\"\n",
    "    plt.rcParams[\"figure.figsize\"] = (6, 6)  # Set the figure size to 8 inches by 6 inches\n",
    "    plt.rcParams[\"font.size\"] = 20  # Set the font size to 12 points\n",
    "    plt.rcParams[\"axes.linewidth\"] = 1.5  # Set the linewidth of the axes to 1.5\n",
    "    #plt.rcParams[\"axes.grid\"] = True  # Show the grid\n",
    "    #plt.rcParams[\"grid.alpha\"] = 0.5  # Set the transparency of the grid to 0.5\n",
    "    #plt.rcParams[\"grid.linewidth\"] = 0.5  # Set the linewidth of the grid to 0.5\n",
    "    plt.rcParams[\"image.cmap\"] = \"viridis\"  # Set the colormap to viridis\n",
    "    plt.rcParams[\"image.interpolation\"] = \"nearest\"  # Set the interpolation to nearest\n",
    "\n",
    "def convert_saxs_to_q(data, pixel_size, wavelength, detector_distance, beam_x, beam_y):\n",
    "    # Define grid of pixel coordinates (assumes data is square)\n",
    "    #x_pixels,y_pixels = data.shape\n",
    "    x_pixels = 487\n",
    "    y_pixels = 195\n",
    "    x_coords = np.arange(x_pixels) - beam_x\n",
    "    y_coords = np.arange(y_pixels) - beam_y\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    # Convert to q-space coordinates\n",
    "    qx = 1e-9*2 * np.pi / wavelength * np.sin(pixel_size * xx / detector_distance)\n",
    "    qy = 1e-9*2 * np.pi / wavelength * np.sin(pixel_size * yy / detector_distance)\n",
    "\n",
    "    print (len(xx))\n",
    "    \n",
    "    return qx, qy\n",
    "\n",
    "\n",
    "    #print (qx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e8cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the poni file and extract relevant parameters\n",
    "poni_file_path = os.path.join(folder_path_poni_mask, WAXS_poni_file)\n",
    "ai = pyFAI.load(poni_file_path)\n",
    "\n",
    "print(ai)\n",
    "\n",
    "\n",
    "\n",
    "#Define detector parameters for 2D q conversion\n",
    "pixel_size = ai.get_pixel1() # in meters \n",
    "wavelength = ai.get_wavelength() #1.54e-10  # in meters\n",
    "detector_distance = ai.get_dist()  # in meters\n",
    "beam_x= 644.072\n",
    "beam_y= 106.130 # beam center x-coordinate in pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Define the base folder name and create the folder\n",
    "\n",
    "folder_path = WAXS_folder_name\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "filename_dx_1 = None \n",
    "filename_dx0 = None\n",
    "filename_dx1 = None\n",
    "\n",
    "for keyword in keywords:\n",
    "    # Create a list of file paths that match the file types and current keyword\n",
    "    raw_file_pattern = f'*{keyword}/WAXS/*.raw'\n",
    "    raw_file_paths = sorted(glob.glob(os.path.join(raw_dir, raw_file_pattern)))\n",
    "\n",
    "    # Initialize the accumulators for the averaged image and pdi data\n",
    "    avg_image = np.zeros((195, 487))\n",
    "\n",
    "    # Initialize variables to store averages\n",
    "    avg_dx_minus_1 = np.zeros((195, 487), dtype=np.int32)\n",
    "    avg_dx_0 = np.zeros((195, 487), dtype=np.int32)\n",
    "    avg_dx_1 = np.zeros((195, 487), dtype=np.int32)\n",
    "\n",
    "    for i, raw_file_path in enumerate(raw_file_paths):\n",
    "        \n",
    "\n",
    "        # Read the raw file\n",
    "        data = np.fromfile(raw_file_path, dtype=np.int32).reshape(195, 487)\n",
    "        avg_image += data\n",
    "\n",
    "        file_name = os.path.splitext(os.path.basename(raw_file_path))[0]\n",
    "\n",
    "        if \"_dx-1_\" in file_name:\n",
    "            #print(file_name)\n",
    "            filename_dx_1 = file_name\n",
    "            filename_dx_1 = filename_dx_1[10:-10]\n",
    "            #print(i)\n",
    "            avg_dx_minus_1 += data\n",
    "        elif \"_dx0_\" in file_name:\n",
    "            avg_dx_0 += data\n",
    "            #print(file_name)\n",
    "            filename_dx0 = file_name\n",
    "            filename_dx0 = filename_dx0[10:-10]\n",
    "        elif \"_dx1_\" in file_name:\n",
    "            avg_dx_1 += data\n",
    "            filename_dx1 = file_name\n",
    "            filename_dx1 = filename_dx1[10:-10]\n",
    "            #print(file_name)\n",
    "        \n",
    "\n",
    "    # Calculate the final averages\n",
    "    dx_1_size=len([f for f in raw_file_paths if \"_dx-1_\" in f])\n",
    "    #print(dx_1_size)\n",
    "    avg_dx_minus_1 = (avg_dx_minus_1 / dx_1_size)\n",
    "    dx0_size = len([f for f in raw_file_paths if \"_dx0_\" in f])\n",
    "    avg_dx_0 = (avg_dx_0 / dx0_size)\n",
    "    dx1_size=len([f for f in raw_file_paths if \"_dx1_\" in f])\n",
    "    avg_dx_1 = (avg_dx_1 /dx1_size)\n",
    "\n",
    "    avg_image /= len(raw_file_paths)\n",
    "\n",
    "\n",
    "    # Assuming raw_file_paths[-1] contains the file path\n",
    "    file_name, file_extension = os.path.splitext(os.path.basename(raw_file_paths[-1]))\n",
    "    \n",
    "    print (file_name)\n",
    "\n",
    "    # Remove the last 10 characters from the file name and the \".raw\" extension\n",
    "    file_name = file_name[10:-10]\n",
    "    file_extension = \"\"\n",
    "\n",
    "    avg_file_name = f\"{file_name}{len(raw_file_paths)}average{file_extension}\"\n",
    "    print(avg_file_name)\n",
    "\n",
    "    #print (avg_file_name)\n",
    "      \n",
    "    # Remove 5 characters from the beginning and 10 characters from the end\n",
    "    #filename_dx_1 = filename_dx_1[5:-10]\n",
    "    #filename_dx0 = filename_dx0[5:-10]\n",
    "    #filename_dx1 = filename_dx1[5:-10]\n",
    "\n",
    "    # Create the modified filenames\n",
    "    avg_dx_1_filename = f\"{filename_dx_1}_{dx_1_size}average\"\n",
    "    #print (avg_dx_1_filename)\n",
    "    avg_dx0_filename = f\"{filename_dx0}_{dx0_size}average\"\n",
    "    avg_dx1_filename = f\"{filename_dx1}_{dx1_size}average\"\n",
    "\n",
    "    \n",
    "    #print (f\"{keyword}_{num_raw_files}_average\")\n",
    "    set_pcolor_properties()\n",
    "    fign, axsn = plt.subplots(figsize=(6,6))\n",
    "    qx,qy=convert_saxs_to_q(avg_image, pixel_size, wavelength, detector_distance, beam_x, beam_y)\n",
    "    #im = axsn.pcolormesh(qx,qy,np.log10(avg_image), cmap='jet')\n",
    "    im = axsn.pcolormesh(-qx, -qy, np.log10(avg_image), cmap='jet')#, vmin=0, vmax=3)\n",
    "    axsn.set_aspect('equal')\n",
    "    cbar=fign.colorbar(im, ax=axsn, shrink=0.4, label='Intensity [a.u.]')\n",
    "\n",
    "\n",
    "    #fign.colorbar(im, ax=axsn)\n",
    "    axsn.set_xlabel(r\"$q_x$ [nm$^{-1}$]\")\n",
    "    axsn.set_ylabel(r\"$q_y$ [nm$^{-1}$]\")\n",
    "    axsn.set_title(f\"{avg_file_name}\",fontsize=10,y=1.3)\n",
    "    # set tick levels\n",
    "    #axsn.set_xticks([-2, -1, 0, 1, 2])\n",
    "    #axsn.set_yticks([-2.4, -1.6, -0.8, 0.0, 0.8, 1.6])\n",
    "    axsn.tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    file_path = os.path.join(folder_path, f\"_{avg_file_name}.png\")\n",
    "    plt.savefig(file_path, dpi=300, bbox_inches='tight', format='png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a figure with a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 7))\n",
    "\n",
    "    # Plot avg_image in the top-left subplot\n",
    "    qx_avg, qy_avg = convert_saxs_to_q(avg_image, pixel_size, wavelength, detector_distance, beam_x, beam_y)\n",
    "    im_avg = axs[0, 0].pcolormesh(-qx_avg, -qy_avg, np.log10(avg_image), cmap='jet')#, vmin=0, vmax=5)\n",
    "    axs[0, 0].set_aspect('equal')\n",
    "    cbar_avg = fig.colorbar(im_avg, ax=axs[0, 0], shrink=0.8, label='Intensity [a.u.]')\n",
    "    axs[0, 0].set_xlabel(r\"$q_x$ [nm$^{-1}$]\")\n",
    "    axs[0, 0].set_ylabel(r\"$q_y$ [nm$^{-1}$]\")\n",
    "    axs[0, 0].set_title(f\"{avg_file_name}\", fontsize=14, y=1.3)\n",
    "    #axs[0, 0].set_xticks([-2, -1, 0, 1, 2])\n",
    "    #axs[0, 0].set_yticks([-2.4, -1.6, -0.8, 0.0, 0.8, 1.6])\n",
    "    axs[0, 0].tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    # Plot avg_dx_minus_1 in the top-right subplot\n",
    "    qx_minus_1, qy_minus_1 = convert_saxs_to_q(avg_dx_minus_1, pixel_size, wavelength, detector_distance, beam_x, beam_y)\n",
    "    im_minus_1 = axs[0, 1].pcolormesh(-qx_minus_1, -qy_minus_1, np.log10(avg_dx_minus_1), cmap='jet')#, vmin=0, vmax=5)\n",
    "    axs[0, 1].set_aspect('equal')\n",
    "    cbar_minus_1 = fig.colorbar(im_minus_1, ax=axs[0, 1], shrink=0.8, label='Intensity [a.u.]')\n",
    "    axs[0, 1].set_xlabel(r\"$q_x$ [nm$^{-1}$]\")\n",
    "    axs[0, 1].set_ylabel(r\"$q_y$ [nm$^{-1}$]\")\n",
    "    axs[0, 1].set_title(\"avg_dx_minus_1\", fontsize=14, y=1.3)\n",
    "    #axs[0, 1].set_xticks([-2, -1, 0, 1, 2])\n",
    "    #axs[0, 1].set_yticks([-2.4, -1.6, -0.8, 0.0, 0.8, 1.6])\n",
    "    axs[0, 1].tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    # Plot avg_dx_0 in the bottom-left subplot\n",
    "    qx_0, qy_0 = convert_saxs_to_q(avg_dx_0, pixel_size, wavelength, detector_distance, beam_x, beam_y)\n",
    "    im_0 = axs[1, 0].pcolormesh(-qx_0, -qy_0, np.log10(avg_dx_0), cmap='jet')#, vmin=0, vmax=5)\n",
    "    axs[1, 0].set_aspect('equal')\n",
    "    cbar_0 = fig.colorbar(im_0, ax=axs[1, 0], shrink=0.8, label='Intensity [a.u.]')\n",
    "    axs[1, 0].set_xlabel(r\"$q_x$ [nm$^{-1}$]\")\n",
    "    axs[1, 0].set_ylabel(r\"$q_y$ [nm$^{-1}$]\")\n",
    "    axs[1, 0].set_title(\"avg_dx_0\", fontsize=14, y=1.3)\n",
    "    #axs[1, 0].set_xticks([-2, -1, 0, 1, 2])\n",
    "    #axs[1, 0].set_yticks([-2.4, -1.6, -0.8, 0.0, 0.8, 1.6])\n",
    "    axs[1, 0].tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    # Plot avg_dx_1 in the bottom-right subplot\n",
    "    qx_1, qy_1 = convert_saxs_to_q(avg_dx_1, pixel_size, wavelength, detector_distance, beam_x, beam_y)\n",
    "    im_1 = axs[1, 1].pcolormesh(-qx_1, -qy_1, np.log10(avg_dx_1), cmap='jet')\n",
    "    axs[1, 1].set_aspect('equal')\n",
    "    cbar_1 = fig.colorbar(im_1, ax=axs[1, 1], shrink=0.8, label='Intensity [a.u.]')\n",
    "    axs[1, 1].set_xlabel(r\"$q_x$ [nm$^{-1}$]\")\n",
    "    axs[1, 1].set_ylabel(r\"$q_y$ [nm$^{-1}$]\")\n",
    "    axs[1, 1].set_title(\"avg_dx_1\", fontsize=14, y=1.3)\n",
    "    #axs[1, 1].set_xticks([-2, -1, 0, 1, 2])\n",
    "    #axs[1, 1].set_yticks([-2.4, -1.6, -0.8, 0.0, 0.8, 1.6])\n",
    "    axs[1, 1].tick_params(axis='both', which='major', labelsize=20, width=1.5)\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #file_path = os.path.join(folder_path, f\"_{avg_file_name}_all.png\")\n",
    "    #plt.savefig(file_path, dpi=50, bbox_inches='tight', format='png')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    # Create a figure with 1 row and 2 columns\n",
    "    fig, axs = plt.subplots(2,1, figsize=(6, 12))\n",
    "\n",
    "    for j, raw_file_path in enumerate(raw_file_paths):\n",
    "        data = np.fromfile(raw_file_path, dtype=np.int32).reshape(195, 487)\n",
    "        file_name = os.path.splitext(os.path.basename(raw_file_path))[0]\n",
    "\n",
    "        q, I, error = ai.integrate1d(data, 1000, error_model='poisson')#, filename=os.path.join(folder_path, f\"{file_name}_SAXS.dat\"))\n",
    "        axs[0].plot(q, I, label=file_name)\n",
    "        #axs[0].set_xlabel(\"q (nm$^{-1}$)\")\n",
    "        axs[0].set_ylabel(\"Intensity (a.u.)\")\n",
    "        #axs[0].set_xlim([0.05, 3])\n",
    "        axs[0].legend(fontsize=12)\n",
    "        axs[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left',fontsize=12)\n",
    "\n",
    "    # Integrate and plot for avg_dx_minus_1\n",
    "    q_dx_minus_1, I_dx_minus_1,error_dx_minus_1 = ai.integrate1d(avg_dx_minus_1, 1000,error_model='poisson', filename=os.path.join(folder_path, f\"{avg_dx_1_filename}_dx_minus_1_WAXS.dat\"))\n",
    "    axs[1].plot(q_dx_minus_1, I_dx_minus_1, '-g',label=f\"{avg_dx_1_filename}_dx_minus_1\")\n",
    "    axs[1].set_xlabel(\"q (nm$^{-1}$)\")\n",
    "    axs[1].set_ylabel(\"Intensity (a.u.)\")\n",
    "    #axs[1].set_xlim([0.05, 3])\n",
    "    axs[1].legend(fontsize=12)\n",
    "\n",
    "    # Integrate and plot for avg_dx_0\n",
    "    q_dx_0, I_dx_0, error_dx_0 = ai.integrate1d(avg_dx_0, 1000,error_model='poisson', filename=os.path.join(folder_path, f\"{avg_dx0_filename}_dx0_WAXS.dat\"))\n",
    "    axs[1].plot(q_dx_0, I_dx_0, '-b',label=f\"{avg_dx0_filename}_dx0\")\n",
    "    axs[1].set_xlabel(\"q (nm$^{-1}$)\")\n",
    "    axs[1].set_ylabel(\"Intensity (a.u.)\")\n",
    "    #axs[1].set_xlim([0.05, 3])\n",
    "    axs[1].legend(fontsize=12)\n",
    "\n",
    "    # Integrate and plot for avg_dx_1\n",
    "    q_dx_1, I_dx_1,error_dx_1 = ai.integrate1d(avg_dx_1, 1000,error_model='poisson', filename=os.path.join(folder_path, f\"{avg_dx1_filename}_dx1_WAXS.dat\"))\n",
    "    axs[1].plot(q_dx_1, I_dx_1, '-k',label=f\"{avg_dx1_filename}_dx1\")\n",
    "    axs[1].set_xlabel(\"q (nm$^{-1}$)\")\n",
    "    axs[1].set_ylabel(\"Intensity (a.u.)\")\n",
    "    #axs[1].set_xlim([0.05, 3])\n",
    "    axs[1].legend(fontsize=12)\n",
    "    axs[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left',fontsize=12)\n",
    "    \n",
    "    q, I, error_avg = ai.integrate1d(avg_image, 1000,error_model='poisson', filename=os.path.join(folder_path, f\"{avg_file_name}_all_WAXS.dat\"))\n",
    "    axs[1].plot(q, I, '-r',label=f\"{avg_file_name}_all\")\n",
    "    axs[1].set_xlabel(\"q (nm$^{-1}$)\")\n",
    "    axs[1].set_ylabel(\"Intensity (a.u.)\")\n",
    "    #axs[1].set_xlim([0.05, 3])\n",
    "    axs[1].legend(fontsize=12)\n",
    "    axs[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left',fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    filename_dx_1 = None \n",
    "    filename_dx0 = None\n",
    "    filename_dx1 = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
